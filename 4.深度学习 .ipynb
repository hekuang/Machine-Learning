{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(3000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 3 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "∂ σ α"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.神经网络\n",
    "\n",
    "### 1.5 线性分界\n",
    "\n",
    "\n",
    "### 1.7 \n",
    "\n",
    " <img src=\"./img/感知器.png\" width=\"60%\">\n",
    " \n",
    "* x 输入\n",
    "* W 权重\n",
    "* b 偏差\n",
    "* label 计算出值后，根据情况赋值 0或者1\n",
    "* 预测(输出/预测公司) y_hat= Wx+b >= 0 y_hat = 1  如果Wx+b<0 y_hat = 0\n",
    "\n",
    "### 1.10 感知器技巧\n",
    "\n",
    "负点进了正区域 （W， b）-α(X，1）\n",
    "正点进了负区域 （W， b）+α(X，1）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感知器算法\n",
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14\n",
    "\n",
    ">误差函数必须是连续的，不能是离散的。\n",
    " \n",
    "### 1.15\n",
    "将激活函数（activation function）离散的阶跃函数（step function） 到 连续的s函数（sigmoid function）\n",
    "### 1.16. Softmax 函数\n",
    "> 概率总和必须是1     \n",
    "\n",
    "激活函数：$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "np.exp()指数e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写 Softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(L):\n",
    "    #     expL = np.exp(L)\n",
    "    #     return np.divide (expL, expL.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.17. One-Hot 编码 \n",
    "yes no no 1 0 0 \n",
    "no yes no 0 10 \n",
    "### 1.18. 最大似然率\n",
    "用a是a的概率，相乘。一直到最大化，最大似然法\n",
    "### 1.19. 最大化概率\n",
    "乘法，改了一个乘数，改变过大。用log转化为加法\n",
    "### 1.20. 交叉熵 corss entropies\n",
    "p1*p2*p3 转化为-ln(p1)-ln(p2)-ln(p3)交叉熵值越大，模型越差  \n",
    "由于y取值为1和0 ，随意直接带入计算  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵公式。\n",
    "#   Y = 1 代表positive Y=0 表示neggtive\n",
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(  Y * np.log(P) + (1 - Y) * np.log(1 - P)  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.22.  多类别交叉熵  \n",
    "Pij i事件在j情况下的概率 Yijln（pij）求和\n",
    "### 1.23. Logistic 回归\n",
    "机器学习中最热门和最有用的算法之一，它也是所有机器学习的基石——对数几率回归算法\n",
    "* 获得数据\n",
    "* 选择一个随机模型\n",
    "* 计算误差\n",
    "* 最小化误差，获得更好的模型 \n",
    "\n",
    "\n",
    "<img src=\"./img/误差函数.png\" width=\"70%\">\n",
    "<img src=\"./img/Error_Function.png\" width=\"40%\" >\n",
    "\n",
    "### 1.24 梯度下降 gradient\n",
    "预测 y = σ(WX + b)\n",
    "梯度 <img src=\"./img/梯度公式.png\" width=\"40%\" >\n",
    "* （∂E/∂wi）=（y-y^)xi  ∂E/∂b=（y-y^)  算出新的权重wi'和新的偏差b'  \n",
    "**w的偏导数就是$（y-\\hat{y})x_i$，b的偏导数就是$（y-\\hat{y})$**\n",
    "* 误差函数\n",
    " $$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "> 梯度实际上是标量乘以点的坐标！ 这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。\n",
    "\n",
    "### 1.25 梯度下降算法 \n",
    "1.随机的权重，w1,,,,wn,b 得出直线Wx+b=0，概率函数σ（Wx+b）\n",
    "2.对已，学习速率α（x1，，，，xn)    \n",
    "* wi-α（∂E/∂wi）得出wi'     \n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "* b-α（∂E/∂b）得出b'    \n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$\n",
    "\n",
    "3.重复，重复次数叫做epoch，一个epoch是指把所有训练数据完整的过一遍\n",
    "<img src=\"./img/梯度下降算法步骤.png\" width=\"40%\" >\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "####  Sigmoid 激活函数   输出（预测）公式  误差函数  更新权重的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-546b3170e344>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-546b3170e344>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    output = output_formula(x, weights, bias)T\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Implement the following functions\n",
    "\n",
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(x))\n",
    "\n",
    "# Output (prediction) formula\n",
    "def output_formula(features, weights, bias):\n",
    "    return  sigmoid(np.matmul(features, weights)+bias)\n",
    "\n",
    "# Error (log-loss) formula\n",
    "def error_formula(y, output):\n",
    "    return -y*(np.log(output))-(1-y)*(np.log(1-output))\n",
    "\n",
    "# Gradient descent step\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)T\n",
    "    d_error = -(y - output)\n",
    "    weights -= learnrate * d_error * x\n",
    "    bias -= learnrate * d_error\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.26. 感知器和梯度下降\n",
    "\n",
    "\n",
    "##### 梯度下降（连续）\n",
    "\n",
    "区别：\n",
    "* 所有点改变权重\n",
    "* $ \\hat{y}$ 是0-1连续取值\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "#### 感知器（离散）\n",
    "区别：\n",
    "* 只有错误点才会改变权重\n",
    "* $ \\hat{y}$ 只能取值0，1\n",
    "$$ w_i \\longrightarrow w_i + \\alpha x_i \\qquad    错误点为 positive \\quad y=1\\ \\hat{y}=0\\quad  y - \\hat{y}=1$$\n",
    "$$ w_i \\longrightarrow w_i - \\alpha x_i \\qquad    错误点为 negative \\quad  y=0\\ \\hat{y}=1\\quad y - \\hat{y}=-1$$ \n",
    "* 分类争取$y - \\hat{y}=0 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程2：深度神经网络\n",
    "### 2.3. 神经网络结构\n",
    "* 输入层input layer &nbsp 　　　　 多节点导致多维空间\n",
    "* 隐藏层hidden layer　　　　　　　多节点，形成深度神经网络\n",
    "* 输出层output layer　　　　　　　输出多节点，多输出\n",
    "### 3.5. 前向反馈 FeedForward\n",
    "预测函数：$\\hat{y}=\\sigma(Wx+b)$  \n",
    "$\\hat{y}=\\sigma(w3 \\sigma (w2\\sigma(w1*x)))$\n",
    "\n",
    "### 3.5. 反向传播 Backpropagation\n",
    "* 进行前向反馈运算。\n",
    "* 将模型的输出与期望的输出进行比较。\n",
    "* 计算误差。\n",
    "* 向后运行前向反馈运算（反向传播），将误差分散到每个权重上。\n",
    "* 更新权重，并获得更好的模型。\n",
    "* 继续此流程，直到获得很好的模型。\n",
    " \n",
    "### 3.5. [Keras](https://keras.io/)\n",
    "**序列模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-970960060049>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Create the Sequential model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    " from keras.models import Sequential\n",
    "\n",
    " #Create the Sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层吧。\n",
    "**层**\n",
    "Keras 层就像神经网络层。有全连接层、最大池化层和激活层。你可以使用模型的 add() 函数添加层。例如，简单的模型可以如下所示：\n",
    "* 激活函数有：relu softmax sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-14ebf63eac67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#创建序列模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "#创建序列模型\n",
    "model = Sequential()\n",
    "\n",
    "# 第一层 - 添加有128个节点的全连接层以及32个节点的输入层\n",
    "# 将维度设为 32（表示数据来自 32 维空间）\n",
    "model.add(Dense(128, input_dim=32))\n",
    "\n",
    "#第二层 - 添加 softmax 激活层\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#第三层 - 添加全连接层\n",
    "model.add(Dense(10))\n",
    "\n",
    "#第四层 - 添加 Sigmoid 激活层\n",
    "model.add(Activation('sigmoid'))\n",
    "# 我们将损失函数 loss 指定为我们一直处理的 categorical_crossentropy。\n",
    "# 我们还可以指定优化程序 optimizer，  adam。\n",
    "# 我们可以指定评估模型用到的指标。我们将使用准确率。\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "# 我们可以使用以下命令来查看模型架构：\n",
    "model.summary()\n",
    "# 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。\n",
    "# 每 epoch 完成对整数据集的一次遍历\n",
    "# verbose 参数可以指定显示训练过程信息类型，这里定义为 0 表示不显示信息\n",
    "# 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。\n",
    "model.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "# 最后，我们可以使用以下命令来评估模型：\n",
    "model.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "# One-hot encoding the output\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "xor.add(Dense(32, input_dim=2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "xor.add(Dense(2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "\n",
    "xor.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "# xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

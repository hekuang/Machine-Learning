{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(3000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 3 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [课程1.神经网络](#1)\n",
    "\n",
    "\n",
    "\n",
    "* [课程2： 深度神经网络](#2)　　\n",
    "　\n",
    " \n",
    " \n",
    "* [课程 3：卷积神经网络](#3)　　\n",
    "\n",
    "\n",
    " \n",
    "* [课程 4：癌症检测深度学习（Sebastian Thrun）](#4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TOC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "∂ σ α\n",
    "training  a neural network,   just means finding the weights we use to calculate the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# [1.神经网络](#0)\n",
    "\n",
    "### 1.5 线性分界\n",
    "\n",
    "\n",
    "### 1.7 \n",
    "\n",
    " <img src=\"./img/感知器.png\" width=\"60%\">\n",
    " \n",
    "* x 输入\n",
    "* W 权重\n",
    "* b 偏差\n",
    "* label 计算出值后，根据情况赋值 0或者1\n",
    "* 预测(输出/预测公司) y_hat= Wx+b >= 0 y_hat = 1  如果Wx+b<0 y_hat = 0\n",
    "\n",
    "### 1.10 感知器技巧\n",
    "\n",
    "负点进了正区域 （W， b）-α(X，1）\n",
    "正点进了负区域 （W， b）+α(X，1）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感知器算法\n",
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14\n",
    "\n",
    ">误差函数必须是连续的，不能是离散的。\n",
    " \n",
    "### 1.15\n",
    "将激活函数（activation function）离散的阶跃函数（step function） 到 连续的s函数（sigmoid function）\n",
    "### 1.16. Softmax 函数\n",
    "> 概率总和必须是1     \n",
    "\n",
    "激活函数：$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "np.exp()指数e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写 Softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(L):\n",
    "    #     expL = np.exp(L)\n",
    "    #     return np.divide (expL, expL.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.17. One-Hot 编码 \n",
    "yes no no 1 0 0 \n",
    "no yes no 0 10 \n",
    "### 1.18. 最大似然率\n",
    "用a是a的概率，相乘。一直到最大化，最大似然法\n",
    "### 1.19. 最大化概率\n",
    "乘法，改了一个乘数，改变过大。用log转化为加法\n",
    "### 1.20. 交叉熵 corss entropies\n",
    "p1*p2*p3 转化为-ln(p1)-ln(p2)-ln(p3)交叉熵值越大，模型越差  \n",
    "由于y取值为1和0 ，随意直接带入计算  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵公式。\n",
    "#   Y = 1 代表positive Y=0 表示neggtive\n",
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(  Y * np.log(P) + (1 - Y) * np.log(1 - P)  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.22.  多类别交叉熵  \n",
    "Pij i事件在j情况下的概率 Yijln（pij）求和\n",
    "### 1.23. Logistic 回归\n",
    "机器学习中最热门和最有用的算法之一，它也是所有机器学习的基石——对数几率回归算法\n",
    "* 获得数据\n",
    "* 选择一个随机模型\n",
    "* 计算误差\n",
    "* 最小化误差，获得更好的模型 \n",
    "\n",
    "\n",
    "<img src=\"./img/误差函数.png\" width=\"70%\">\n",
    "<img src=\"./img/Error_Function.png\" width=\"40%\" >\n",
    "\n",
    "### 1.24 梯度下降 gradient\n",
    "预测 y = σ(WX + b)\n",
    "梯度 <img src=\"./img/梯度公式.png\" width=\"40%\" >\n",
    "* （∂E/∂wi）=（y-y^)xi  ∂E/∂b=（y-y^)  算出新的权重wi'和新的偏差b'  \n",
    "**w的偏导数就是$（y-\\hat{y})x_i$，b的偏导数就是$（y-\\hat{y})$**\n",
    "* 误差函数\n",
    " $$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "> 梯度实际上是标量乘以点的坐标！ 这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。\n",
    "\n",
    "### 1.25 梯度下降算法 \n",
    "1.随机的权重，w1,,,,wn,b 得出直线Wx+b=0，概率函数σ（Wx+b）\n",
    "2.对已，学习速率α（x1，，，，xn)    \n",
    "* wi-α（∂E/∂wi）得出wi'     \n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "* b-α（∂E/∂b）得出b'    \n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$\n",
    "\n",
    "3.重复，重复次数叫做epoch，一个epoch是指把所有训练数据完整的过一遍\n",
    "<img src=\"./img/梯度下降算法步骤.png\" width=\"40%\" >\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "####  Sigmoid 激活函数   输出（预测）公式  误差函数  更新权重的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-546b3170e344>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-546b3170e344>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    output = output_formula(x, weights, bias)T\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Implement the following functions\n",
    "\n",
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(x))\n",
    "\n",
    "# Output (prediction) formula\n",
    "def output_formula(features, weights, bias):\n",
    "    return  sigmoid(np.matmul(features, weights)+bias)\n",
    "\n",
    "# Error (log-loss) formula\n",
    "def error_formula(y, output):\n",
    "    return -y*(np.log(output))-(1-y)*(np.log(1-output))\n",
    "\n",
    "# Gradient descent step\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)T\n",
    "    d_error = -(y - output)\n",
    "    weights -= learnrate * d_error * x\n",
    "    bias -= learnrate * d_error\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.26. 感知器和梯度下降\n",
    "\n",
    "\n",
    "##### 梯度下降（连续）\n",
    "\n",
    "区别：\n",
    "* 所有点改变权重\n",
    "* $ \\hat{y}$ 是0-1连续取值\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "#### 感知器（离散）\n",
    "区别：\n",
    "* 只有错误点才会改变权重\n",
    "* $ \\hat{y}$ 只能取值0，1\n",
    "$$ w_i \\longrightarrow w_i + \\alpha x_i \\qquad    错误点为 positive \\quad y=1\\ \\hat{y}=0\\quad  y - \\hat{y}=1$$\n",
    "$$ w_i \\longrightarrow w_i - \\alpha x_i \\qquad    错误点为 negative \\quad  y=0\\ \\hat{y}=1\\quad y - \\hat{y}=-1$$ \n",
    "* 分类争取$y - \\hat{y}=0 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# [课程2：深度神经网络](#0)\n",
    "### 2.3. 神经网络结构\n",
    "* 输入层input layer &nbsp 　　　　 多节点导致多维空间\n",
    "* 隐藏层hidden layer　　　　　　　多节点，形成深度神经网络\n",
    "* 输出层output layer　　　　　　　输出多节点，多输出\n",
    "### 2.5. 前向反馈 FeedForward\n",
    "预测函数：$\\hat{y}=\\sigma(Wx+b)$  \n",
    "$\\hat{y}=\\sigma(w3 \\sigma (w2\\sigma(w1*x)))$\n",
    "\n",
    "### 2.6. 反向传播 Backpropagation\n",
    "* 进行前向反馈运算。\n",
    "* 将模型的输出与期望的输出进行比较。\n",
    "* 计算误差。\n",
    "* 向后运行前向反馈运算（反向传播），将误差分散到每个权重上。\n",
    "* 更新权重，并获得更好的模型。\n",
    "* 继续此流程，直到获得很好的模型。\n",
    " \n",
    "### 2.7. [Keras](https://keras.io/)\n",
    "**序列模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-970960060049>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Create the Sequential model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    " from keras.models import Sequential\n",
    "\n",
    " #Create the Sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层吧。\n",
    "**层**\n",
    "Keras 层就像神经网络层。有全连接层、最大池化层和激活层。你可以使用模型的 add() 函数添加层。例如，简单的模型可以如下所示：\n",
    "* 激活函数有：relu softmax sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-14ebf63eac67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#创建序列模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "#创建序列模型\n",
    "model = Sequential()\n",
    "\n",
    "# 第一层 - 添加有128个节点的全连接层以及32个节点的输入层\n",
    "# 将维度设为 32（表示数据来自 32 维空间）\n",
    "model.add(Dense(128, input_dim=32))\n",
    "\n",
    "#第二层 - 添加 softmax 激活层\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#第三层 - 添加全连接层\n",
    "model.add(Dense(10))\n",
    "\n",
    "#第四层 - 添加 Sigmoid 激活层\n",
    "model.add(Activation('sigmoid'))\n",
    "# 我们将损失函数 loss 指定为我们一直处理的 categorical_crossentropy。\n",
    "# 我们还可以指定优化程序 optimizer，  adam。\n",
    "# 我们可以指定评估模型用到的指标。我们将使用准确率。\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "# 我们可以使用以下命令来查看模型架构：\n",
    "model.summary()\n",
    "# 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。\n",
    "# 每 epoch 完成对整数据集的一次遍历\n",
    "# verbose 参数可以指定显示训练过程信息类型，这里定义为 0 表示不显示信息\n",
    "# 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。\n",
    "model.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "# 最后，我们可以使用以下命令来评估模型：\n",
    "model.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "# One-hot encoding the output\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "xor.add(Dense(32, input_dim=2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "xor.add(Dense(2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "\n",
    "xor.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "# xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. 早期停止\n",
    "> 降低梯度，直到测试误差停止降低并开始增大\n",
    "\n",
    "### 2.12. 正则化\n",
    "\n",
    "<img src='./img/正则化.png' width = '70%'>\n",
    "### 2.14. Dropout\n",
    "\n",
    "> 每次drop一定量的节点\n",
    "\n",
    "### 2.15. Dropout\n",
    "### 2.16. 梯度消失\n",
    "Relu激活函数\n",
    "### 2.17. 其他激活函数\n",
    "* 双曲正切 Hyperbolic Tangent [-1,1 ]  　　 $tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "* 修正线性单元ReLU，最后依旧是s函数 　　 $ relu(x) = \\begin{cases} x, & \\text {if $x\\geq0$ } \\\\ 0, & \\text{if $x<0$  } \\end{cases} $\n",
    "### 2.18. 批次与随机梯度下降 \n",
    "> 误差函数的梯度负值方向，epochs次数=步长次数（number of steps），每次都是所以数据\n",
    "\n",
    "而批次，指把数据分为几个批次，每一个批次是一个epochs\n",
    "### 2.19. 学习速率衰退\n",
    "* 过大的学习速率，大步长 ，可能错过最低点，可能错过最佳点 **当模型不可行，适当降低学习速率**\n",
    "* 过小的学习速率，小过长，模型速度过慢\n",
    "### 2.20. 随机重新开始\n",
    "> 从不同地方开始，得到最低点可能性增加\n",
    "\n",
    "### 2.21. 动量Momentun\n",
    "\n",
    "$\\beta$ [0.1 ]\n",
    "step(n) + $\\beta$step(n-1))+$\\beta^2$step(n-2)+...\n",
    "### 2.22 Keras 中的优化程序\n",
    "##### SGD\n",
    "这是随机梯度下降。它使用了以下参数：\n",
    "\n",
    "* 学习速率。\n",
    "* 动量（获取前几步的加权平均值，以便获得动量而不至于陷在局部最低点）。\n",
    "* Nesterov 动量（当最接近解决方案时，它会减缓梯度）。\n",
    "##### Adam\n",
    "Adam (Adaptive Moment Estimation) 使用更复杂的指数衰减，不仅仅会考虑平均值（第一个动量），并且会考虑前几步的方差（第二个动量）。\n",
    "##### RMSProp\n",
    "RMSProp (RMS 表示均方根误差）通过除以按指数衰减的平方梯度均值来减小学习速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "\n",
    "# [课程 3：卷积神经网络](#0)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 计算机是如何解析图片的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# use Keras to import pre-shuffled MNIST database   \n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=False)  \n",
    "X_train ,y_train= mnist.train.images,mnist.train.labels \n",
    "print(X_train.shape)\n",
    "X_train.shape = [55000,28,28]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f64d9d6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# plot first six training images\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])\n",
    "   \n",
    "    ax.imshow(X_train[i], cmap='gray')\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 图片分类MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 MLP 何时效果不错？\n",
    "CNN 在处理多维数据是比MLP效果好，MLP会吧多维转化为向量，不利于处理多数据\n",
    "<img src='./img/CNN与MLP比较.png'  width='77%'>\n",
    "\n",
    "### 3.8. 局部连接性\n",
    "完全连接层（fully connected layer） to 局部连接层（locally connected layer） \n",
    "### 3.9. 卷积层 \n",
    "> convolutional layer is a stack of feature(特征映射堆栈)\n",
    " one feature map for each filter\n",
    " \n",
    " \n",
    "* 后一层使用更深层次的卷积层，可以利用前一层卷积层中发现规律，来找出更深层的规律\n",
    "\n",
    "Filter 检测特征，可视化Filter\n",
    "ReLU\n",
    "不设定权重wights让网络判断哪些权重可以和最小化损失函数loss function\n",
    "\n",
    "集合collections either feature maps（特征映射） or activation maps(激活映射)\n",
    "* 滤波器，过滤器\n",
    "   * 灰度图片，二维\n",
    "   * 彩色图片，三维，filter has three dimensional, 滤波器的重叠，\n",
    "\n",
    "### 3.11. Stride 和填充\n",
    "* set padding to 'vaild'   lose some nodes in the convolutional layer\n",
    "* set padding to 'same'\n",
    "### 3.12. Keras 中的卷积层\n",
    "1.导入模块  \n",
    "`from keras.layers import Conv2D `\n",
    "2.创建卷积层  \n",
    "`Conv2D(filtrers, kernel.size, strides, padding, activation='relu',input_shape)`\n",
    "3.参数\n",
    "\n",
    "* 必须传递以下参数：\n",
    "   * filters - 过滤器数量。\n",
    "   * kernel_size - 指定（方形）卷积窗口的高和宽的数字。\n",
    "* 你可能还需要调整其他可选参数：\n",
    "   * strides - 卷积 stride。如果不指定任何值，则 strides 设为 1。\n",
    "   * padding - 选项包括 'valid' 和 'same'。如果不指定任何值，则 padding 设为 'valid'。\n",
    "   * activation - 通常为 'relu'。如果未指定任何值，则不应用任何激活函数。强烈建议你向网络中的每个卷积层添加一个 ReLU 激活函数\n",
    "* 注意：\n",
    "   * 可以将 kernel_size 和 strides 表示为数字或元组。\n",
    "\n",
    "   * 在模型中将卷积层当做第一层级（出现在输入层之后）时，必须提供另一个 input_shape 参数： input_shape - 指定输入的高度、宽度和深度（按此顺序）的元组。注意：如果卷积层不是网络的第一个层级，请勿包含 input_shape 参数。\n",
    "   \n",
    "### 3.13. 维度\n",
    "\n",
    "* Param #  卷积层具有 80 个参数。\n",
    "* Output Shape   None 对应的是批次大小，卷积层的高度为 100，宽度为 100，深度为 16。\n",
    "* 公式：卷积层中的参数数量\n",
    "> 卷积层中的参数数量取决于 filters、kernel_size 和 input_shape 的值。我们定义几个变量：\n",
    "\n",
    "   * K - 卷积层中的过滤器数量   K = filters，\n",
    "   * F - 卷积过滤器的高度和宽度   F = kernel_size\n",
    "   * D_in - 上一层级的深度    D_in 是 input_shape 元祖中的最后一个值。\n",
    " \n",
    "\n",
    "因为每个过滤器有 F\\*F\\*D_in 个权重，卷积层由 K 个过滤器组成，因此卷积层中的权重总数是 K\\*F\\*F\\*D_in。因为每个过滤器有 1 个偏差项，卷积层有 K 个偏差。\n",
    "\n",
    "因此，卷积层中的**参数数量**是 K\\*F\\*F\\*D_in + K。\n",
    "\n",
    "* 公式：卷积层的形状\n",
    "> 卷积层的形状取决于 kernel_size、input_shape、padding 和 stride 的值。我们定义几个变量：\n",
    "\n",
    "   * K - 卷积层中的过滤器数量   K = filters、\n",
    "   * F - 卷积过滤器的高度和宽度   F = kernel_size\n",
    "   * H_in - 上一层级的高度\n",
    "   * W_in - 上一层级的宽度\n",
    "   * S = stride。\n",
    " 类似地，H_in 和 W_in 分别是 input_shape 元祖的第一个和第二个值。\n",
    "\n",
    "卷积层的**深度**始终为过滤器数量 K。\n",
    "\n",
    "* 如果 padding = 'same'，那么卷积层的空间维度如下：\n",
    "  * (height = ceil(float(H_in) / float(S))\n",
    "  * width = ceil(float(W_in) / float(S))\n",
    "  \n",
    "* 如果 padding = 'valid'，那么卷积层的空间维度如下:\n",
    "\n",
    "   * height = ceil(float(H_in - F + 1) / float(S))\n",
    "   * width = ceil(float(W_in - F + 1) / float(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 8)         40        \n",
      "=================================================================\n",
      "Total params: 40\n",
      "Trainable params: 40\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=8, kernel_size=2, strides=8, padding='valid', \n",
    "    activation='relu', input_shape=(200, 200, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13. 池化层 pooling layer\n",
    "> often take convolutional layers(卷积层) as input\n",
    "复杂数据及——大量过滤器（负责找一种规律）——高维度——多参数——过拟合\n",
    "\n",
    "\n",
    "* pool_size - 指定池化窗口高度和宽度的数字。（必须指定）\n",
    "\n",
    "* strides - 垂直和水平 stride。  默认为 pool_size。\n",
    "* padding - 选项包括 'valid' 和 'same'。 默认 padding 设为 'valid'。\n",
    "      注意：可以将 pool_size 和 strides 表示为数字或元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入\n",
    "from keras.layers import MaxPooling2D\n",
    "# 创建卷积层\n",
    "MaxPooling2D(pool_size,strides, padding) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling2d_7 (MaxPooling2 (None, 50, 50, 15)        0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.16. 图片分类CNN\n",
    "* 注意事项：\n",
    "   * 始终向 CNN 中的 Conv2D 层添加 ReLU 激活函数。但是网络的最后层级除外，密集层也应该具有 ReLU 激活函数。\n",
    "   * 在构建分类网络时，网络中的最后层级应该是具有 softmax 激活函数的 密集层。最后层级的节点数量应该等于数据集中的类别总数。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 528,054\n",
      "Trainable params: 528,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 和神经网络一样，我们通过首先创建一个序列模型来创建一个 CNN。\n",
    "from keras.models import Sequential\n",
    "\n",
    "# 导入几个层，包括熟悉的神经网络层，以及在这节课学习的新的层。\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 和神经网络一样，通过使用 .add() 方法向网络中添加层级：\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.21 可视化CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3.23. 迁移学习\n",
    "> 对训练过的神经网络调整，用于新的不同的数据集\n",
    "1.setting numerous hyper parameters\n",
    "2.choose last function and optimizer\n",
    "\n",
    "\n",
    "* 删掉后几层，保留前几层。\n",
    "* 对小型数据集使用迁移学习需要考虑过拟合现象。\n",
    "\n",
    "* 下面是卷积神经网络的作用一般概述：\n",
    "  * 第一层级将检测图片中的边缘\n",
    "  * 第二层级将检测形状\n",
    "  * 第三个卷积层将检测更高级的特征\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "\n",
    "# [ 课程 4：癌症检测深度学习（Sebastian Thrun）](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 清洗数据很重要\n",
    "* 对网络训练完全不同的事物(如检测皮肤癌，训练猫狗汽车图片），最终可能得到更好的结果\n",
    "\n",
    "*\n",
    "   * Recall 查全率 Recall = True Positives / ( True Positives + False Negatives)\n",
    "   * Precison 查准率 Precision = True Positives / (True Positives + False Positives)\n",
    "   * Sentitivity  敏感性   就是Recall  sensitive to cancer means it finds cancer\n",
    "   * Specificity 特异性    不是查准率 \n",
    "   \n",
    " \n",
    "* 定义\n",
    "   * 敏感性：在患有癌症的所有人中，诊断正确的人有多少？\n",
    "  * 特异性：在未患癌症的所有人中，诊断正确的人有多少？\n",
    "  * 查全率：在被诊断患有癌症的所有人中，多少人确实得了癌症？\n",
    "  * 查准率：在患有癌症的所有人中，多少人被诊断患有癌症？\n",
    "* 标记 注：T代表诊断正确，F错误。P说有病，N说没病  \n",
    "  * TP：（真阳性）被正确诊断为患病的病人。\n",
    "  * TN：（真阴性）被正确诊断为健康的健康人。\n",
    "  * FP：（假阳性）被错误诊断为患病的健康人。\n",
    "  * FN：（假阴性）被错误诊断为健康的病人。\n",
    "* ROC\n",
    "  * 敏感性：在所有恶性病变中，位于阈值右侧的病变（正确分类）的百分比是多少？\n",
    "  * 特异性：在所有良性病变中，位于阈值左侧的病变（正确分类）的百分比是多少？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/an46/Dog-Breed-Recognition-Project/blob/master/dog_app.ipynb\n",
    "https://github.com/vaklyuenkov/aind-cnn-dog_project/blob/master/dog_app.ipynb\n",
    "https://github.com/loeiten/dog-project/blob/master/dog_app.ipynb\n",
    "https://blog.csdn.net/weixin_41770169/article/details/80498138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

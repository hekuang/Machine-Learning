{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(3000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 3 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.神经网络\n",
    "\n",
    "### 1.5 线性分界\n",
    "\n",
    "Wx + b = 0 W = (w1, w2) x=(x1,x2)\n",
    "\n",
    "### 1.7 \n",
    "\n",
    " <img src=\"./img/感知器.png\" width=\"60%\">\n",
    " \n",
    " * x 输入\n",
    "* W 权重\n",
    "* b 偏差\n",
    "* label 计算出值后，根据情况赋值 0或者1\n",
    "* 预测 y_hat Wx+b >= 0 y_hat = 1  如果Wx+b<0 y_hat = 0\n",
    "\n",
    "### 1.10 感知器技巧\n",
    "\n",
    "负点进了正区域 （W， b）-α(X，1）\n",
    "正点进了负区域 （W， b）+α(X，1）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感知器算法\n",
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14\n",
    "\n",
    ">误差函数必须是连续的，不能是离散的。\n",
    " \n",
    "### 1.15\n",
    "将激活函数（activation function）离散的阶跃函数（step function） 到 连续的s函数（sigmoid function）\n",
    "### 1.16. Softmax 函数\n",
    "> 概率总和必须是1     \n",
    "\n",
    "exp\n",
    "\n",
    "np.exp()指数e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写 Softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(L):\n",
    "    #     expL = np.exp(L)\n",
    "    #     return np.divide (expL, expL.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.17. One-Hot 编码 \n",
    "yes no no 1 0 0 \n",
    "no yes no 0 10 \n",
    "### 1.18. 最大似然率\n",
    "用a是a的概率，相乘。一直到最大化，最大似然法\n",
    "### 1.19. 最大化概率\n",
    "乘法，改了一个乘数，改变过大。用log转化为加法\n",
    "### 1.20. 交叉熵 corss entropies\n",
    "p1*p2*p3 转化为-ln(p1)-ln(p2)-ln(p3)交叉熵值越大，模型越差  \n",
    "由于y取值为1和0 ，随意直接带入计算  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵公式。\n",
    "#   Y = 1 代表positive Y=0 表示neggtive\n",
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(  Y * np.log(P) + (1 - Y) * np.log(1 - P)  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.21.  多类别交叉熵  \n",
    "Pij i事件在j情况下的概率 Yijln（pij）求和\n",
    "### 1.22. Logistic 回归\n",
    "机器学习中最热门和最有用的算法之一，它也是所有机器学习的基石——对数几率回归算法\n",
    "* 获得数据\n",
    "* 选择一个随机模型\n",
    "* 计算误差\n",
    "* 最小化误差，获得更好的模型\n",
    "\n",
    "<img src=\"./img/误差函数.png\" width=\"70>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

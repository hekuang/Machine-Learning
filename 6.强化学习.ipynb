{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [课程1.强化学习框架：问题](#1)\n",
    "\n",
    "\n",
    "\n",
    "* [课程2：强化学习框架：解决方案](#2)　　\n",
    "　\n",
    " \n",
    " \n",
    "* [课程 3：动态规划](#3)　　\n",
    "\n",
    "\n",
    " \n",
    "* [课程 4：蒙特卡洛方法](#4)|\n",
    "\n",
    "\n",
    "* [课程 5： 时间差方法](#5)\n",
    "\n",
    "\n",
    "* [课程6 ：连续空间中的强化学习](#6)\n",
    "\n",
    "\n",
    "* [课程7 ：Deep Q-learning](#6)\n",
    "\n",
    "\n",
    "* [课程8：策略梯度](#6)\n",
    "\n",
    "\n",
    "* [课程9： 行动者，评论者方法](#6)\n",
    "\n",
    "\n",
    "* [项目：创建客户细分](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.cnblogs.com/jinxulin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '1'></a>\n",
    "# [1.强化学习框架：问题](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**动作action，状态state，奖励reward**\n",
    "* states : a context provided to the agent for choosing intelligent actions \n",
    "<img src = \"./img/强化学习中的智能体环境互动.png\" width = \"44%\">\n",
    "#### 设置，重新经历\n",
    "* 强化学习 (RL) 框架包含学习与其**环境**互动的**智能体**。\n",
    "* 在每个时间步，智能体都收到环境的**状态**（环境向智能体呈现一种情况），智能体必须选择相应的响应**动作**。一个时间步后，智能体获得一个**奖励**（环境表示智能体是否对该状态做出了正确的响应）和新的**状态**。\n",
    "* 所有智能体的目标都是**最大化预期累积奖励**，或在所有时间步获得的预期奖励之和。\n",
    " \n",
    "### 1.3阶段性任务和连续性任务\n",
    "* **任务**是一种强化学习问题\n",
    "* **episodic tasks** : a well-defined ending point( 起始点，结束点明确，每当智能体抵达**最终状态**,阶段性任务都会结束)\n",
    "* **continuing tasks** : go on forever,without end.\n",
    "\n",
    "\n",
    "### 1.6 奖励假设\n",
    "\n",
    "> 奖励假设：所有目标都可以构建为最大化（预期）累积奖励。\n",
    "\n",
    "Reward Hypothesis ： **maximization of expected cumulative reward (最大化期望累积奖励)**  rely on a prediction or an estimate\n",
    "\n",
    "\n",
    "### 1.7 目标与奖励\n",
    "\n",
    "<img  src =\"./img/目标与奖励.png\" width = \"33%\"  >\n",
    "<img src = \"./img/6.1.7.reward.png\" width = \"55%\">\n",
    "\n",
    "\n",
    "### 1.10.累积奖励\n",
    "> the agent always has reward at all time steps in mind\n",
    "\n",
    "return-G: the sum of rewards from the next time step  (后续时间步的奖励之和)\n",
    "* 在时间步t的回报是：$ G_t = R_{t+1}+ R_{t+2}+ R_{t+3}+....$\n",
    "\n",
    "\n",
    "### 1.11. 折扣汇报\n",
    "> t时刻之后未来执行一组acton后能够获得的reward\n",
    "\n",
    "discounted return: particularly relevant to contimuing tasks(有连续性任务的关系最大)\n",
    "discount rata $\\gamma \\in [0,1]$ (set by youself to refine the goal)\n",
    "> 尤其，当我们指代“回报”时，并不一定就是\\gamma = 1γ=1，当我们指代“折扣回报”时，并不一定就是\\gamma < 1γ<1。 \n",
    "\n",
    "\n",
    "$$ G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+....=\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}$$\n",
    "\n",
    "### 1.12 折扣率的选取\n",
    "* much closer to one\n",
    "* 折扣率的选取带来更多奖励,\n",
    "* 折扣回报 $\\gamma$ 是你设置的值，以便进一步优化智能体的目标。\n",
    "* $它必须指定 0 \\leq \\gamma \\leq 1。$\n",
    "* $如果 \\gamma=0，智能体只关心最即时的奖励。$\n",
    "* $如果 \\gamma=1，回报没有折扣。$\n",
    "* $\\gamma 的值越大，智能体越关心遥远的未来。\\gamma 的值越小，折扣程度越大，在最极端的情况下，智能体只关心最即时的奖$\n",
    "\n",
    "\n",
    "### 1.13.MDP（Markov Decision Process）马尔科夫决策过程\n",
    "> 环境做出决策需要的信息很少\n",
    "\n",
    "<img src = \"./img/MDP_definition.png\" width = \"66%\">\n",
    "* actions -- **he are thing that  you can do** action space denote with a script A (手写体A)\n",
    "* states  -- **describe the world** state space  denote with a script S\n",
    "* model --  **rules of the game that you're playing** $T(s,a,s\\prime)$\n",
    "\n",
    "non-deterministic world:\n",
    "\n",
    "\n",
    "$\\mathcal S$ : 状态空间——是指所有非终止状态集合。连续性任务中，就相当于所有状态集合。     \n",
    "$\\mathcal{S}^+$ ： 阶段性任务中， 表示**包括终止状态**所有状态集合  \n",
    "$\\mathcal A$ ： 动作空间——智能体可以采取的动作集合。  \n",
    "$\\mathcal{A}(s)$ ： 状态 $s\\in\\mathcal{S}$下可以采取的动作集合  \n",
    "$\\mathcal{P}$ : 转移状态矩阵\n",
    "\n",
    "MDP： this framework works for continuing and episodic tasks\n",
    "\n",
    "\n",
    "### 1.15 一步动态特性\n",
    "\n",
    "> one-step dynamics of the environment\n",
    "\n",
    "实际上，在任何状态$ S_{t}  和动作 A_{t} ，都可以使用该图判断智能体将如何确定下个状态 S_{t+1} 和奖励 R_{t+1}$\n",
    "\n",
    "在随机时间步t，智能体环境互动变成一系列的状态、动作和奖励。  \n",
    "当环境在时间步 t+1 对智能体做出响应时，它只考虑上一个时间步 $(S_t, A_t )$ 的状态和动作。  \n",
    "尤其是，它不关心再上一个时间步呈现给智能体的状态。（换句话说，环境不考虑任何 $ S_0, \\ldots, S_{t-1}$。）  \n",
    "并且，它不考虑智能体在上个时间步之前采取的动作。（换句话说，环境不考虑任何 $A_0, \\ldots, A_{t-1} $。）  \n",
    "此外，智能体的表现如何，或收集了多少奖励，对环境选择如何对智能体做出响应没有影响。（换句话说，环境不考虑任何 $ R_0, \\ldots, R_t $ 。)\n",
    "\n",
    "因此，我们可以通过指定以下设置完全定义环境如何决定状态和奖励\n",
    "$$p(s′,r∣s,a)≐P(S_{t+1}=s ′ ,R_{t+1}\t =r∣S_t\t =s,A_t\t =a)$$\n",
    "\n",
    "### 1.18. 有限MDP\n",
    "\n",
    "一个（有限）马尔可夫决策过程 (MDP) 由以下各项定义：\n",
    "* 一组（有限的）状态 $\\mathcal{S}$（对于阶段性任务，则是 $\\mathcal{S}^+$）\n",
    "* 一组（有限的）动作$ \\mathcal{A}$\n",
    "* 一组奖励 $\\mathcal{R}$\n",
    "* 环境的一步动态特性\n",
    "* 折扣率$ \\gamma \\in [0,1] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2'></a>\n",
    "# [2. 强化学习框架：解决方案](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 策略policy $\\pi$\n",
    "**Definition**:    \n",
    "1, A deterministic policy is a mapping(映射) $\\pi : \\mathcal{S}---> \\mathcal{A}$\n",
    "* income  : state\n",
    "* outcome : action\n",
    "\n",
    "2, A stochastic policy(随机性策略) is a mapping:  $\\pi : \\mathcal{S} \\times \\mathcal{A} --> [0,1]$\n",
    "> allow the agent to choose actions randomly  \n",
    "\n",
    "$$ \\pi(a|s) = P (A_t = a| S_t = s)$$\n",
    "* income: staste-s,action-a\n",
    "* outcome: probability that the agent takes action a while in state s\n",
    "\n",
    "### 2.4. 网络世界实例\n",
    "<img src = \"./img/网络世界.png\" width = \"44%\">\n",
    "\n",
    "### 2.5. 状态值函数$v_\\pi(s)$\n",
    "\n",
    "* **state-value function** :a function of the environment state \n",
    "> For each state, the **state-value function** yields the expected return ,if the agent started in that state,and then followed the policy for all times steps.(从当前状态开始，并遵循该策略可能会获得的回报)\n",
    "\n",
    "<img src=\"./img/state-value_function.png\" width=\"55%\">\n",
    "\n",
    "### 2.6.贝尔曼方程(1)\n",
    "贝尔曼方程（Bellman Equation）  也被称作动态规划方程（Dynamic Programming Equation）\n",
    " \n",
    "#### 贝尔曼方程\n",
    "在这个网格世界示例中，一旦智能体选择一个动作，\n",
    "\n",
    "* 它始终沿着所选方向移动（而一般 MDP 则不同，智能体并非始终能够完全控制下个状态将是什么）\n",
    "* 可以确切地预测奖励（而一般 MDP 则不同，奖励是从概率分布中随机抽取的）。\n",
    " \n",
    "任何状态s的值可以计算为即时奖励r和下个状态（折扣）值s'的**和**。\n",
    "\n",
    " 一般 MDP，我们需要使用期望值，因为通常即时奖励和下个状态无法准确地预测。 奖励和下个状态是根据 MDP 的一步动态特性选择的。在这种情况下，奖励 r 和下个状态 s' 是从（条件性）概率分布 p(s',r|s,a) 中抽取的，贝尔曼预期方程（对于 $v_\\pi$）表示了任何状态 s 对于预期即时奖励和下个状态的预期值的值：\n",
    "$$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    "\n",
    "#### 计算预期值\n",
    "1.如果智能体的策略 $\\pi$ 是**确定性策略**，智能体在状态 s 选择动作 $\\pi(s)$，贝尔曼预期方程可以重写为两个变量 $(s'  和 r) $的和：\n",
    "$$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s'))$$\n",
    "\n",
    "在这种情况下，我们将奖励和下个状态的折扣值之和$ (r+\\gamma v_\\pi(s'))$ 与相应的概率$ p(s',r|s,\\pi(s)) $相乘，并将所有概率相加得出预期值。\n",
    "\n",
    "2.如果智能体的策略 $\\pi$  是**随机性策略**，智能体在状态 s 选择动作 a 的概率是$ \\pi(a|s$），贝尔曼预期方程可以重写为三个变量（s' 、r 和 a）的和：$$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))$$\n",
    "在这种情况下，我们将奖励和下个状态的折扣值之和$ (r+\\gamma v_\\pi(s'))$ 与相应的概率 $\\pi(a|s)p(s',r|s,a)$ 相乘，并将所有概率相加得出预期值。\n",
    "### 2.8. 最优性\n",
    "**Definition**: \n",
    "$\\pi\\geq\\pi^\\prime \\; only\\; if \\; v_{\\pi^\\prime}(s)\\geq v_{\\pi} \\; for\\;  all \\; \\mathcal{s}\\in \\mathcal{S}$\n",
    "\n",
    "An optimal policy(最优策略) $\\pi_*$ satisfies  $\\pi_* \\geq \\pi$  for all $\\pi$\n",
    "\n",
    "$\\pi$ is a optimal policy , mostly it's not the only optimal policy\n",
    "\n",
    "**The optimal state-value function :** all optimal policies have the **same value function** which we dnote by  $\\mathcal{V}_*$（最优状态值函数）\n",
    "\n",
    "### 2.9. 动作值函数$q_\\pi(s,a)$\n",
    "* **action-value function** :a function of the environment state and the agent's action\n",
    "> For each state and action. the action-value function yields the expected return, if the agent starts in that state, **takes the action** ,and then follows the policy for all future time steps（做出预定动作后，从当前状态开始，并遵循该策略可能会获得的回报，reward包括第一步)\n",
    "\n",
    "**The optimal action-value function (最优动作值函数)**is denoted $\\mathcal{q}_*$\n",
    "\n",
    "<img src=\"./img/状态值函数vs动作值函数.png\" width = \"88%\">\n",
    "\n",
    "### 2.11.  最优策略\n",
    "\n",
    "<img src = \"./img/最优策略.png\" width=\"88%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13. 贝尔曼方程（2）\n",
    " \n",
    "\n",
    "#### 贝尔曼预期方程\n",
    "- 表明当前状态的值函数与下一个状态的值函数的关系\n",
    "\n",
    " $$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s$$\n",
    " $$q_\\pi(s,a) = \\text{}\\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1})|S_t =s, A_t=a]$$\n",
    " \n",
    "#### 贝尔曼最优性方程\n",
    "\n",
    " $$v_*(s) = \\max_{a \\in \\mathcal{A}(s)} \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t=s] $$\n",
    " $$q_*(s,a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'\\in\\mathcal{A}(S_{t+1})}q_*(S_{t+1},a') | S_t=s, A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意:\n",
    ">强化学习的目的就是求解马尔可夫决策过程(MDP)的最优策略，使其在任意初始状态下，都能获得最大的Vπ值。 \n",
    "那么如何求解最优策略呢？基本的解法有三种：\n",
    "\n",
    "- 动态规划法(dynamic programming methods)\n",
    "\n",
    "- 蒙特卡罗方法(Monte Carlo methods)\n",
    "\n",
    "- 时间差分法(temporal difference)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"3\"></a>\n",
    "# [3.动态规划](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 简介\n",
    "> 在动态规划设置中，智能体完全了解表示环境特性的马尔可夫决策流程 (MDP)。（这比强化学习设置简单多了，在强化学习设置中，智能体一开始不知道环境如何决定状态和奖励，必须完全通过互动学习如何选择动作。）\n",
    "\n",
    "### 3.2. 迷你项目\n",
    "`https://viewae1e8f24.udacity-student-workspaces.com/notebooks/Dynamic_Programming-zh.ipynb`\n",
    "\n",
    " ### 3.4-5 迭代方法\n",
    " \n",
    "directly solving the value of all  state  is more diffficult   \n",
    "so start off with a guess for the value of each state, and using an **iterative迭代 method** \n",
    "### 3.7. 迭代策略评估 iterative policy evaluation\n",
    "<img src = \"./img/IterativePolicyEvaluation.png\" width= \"66%\">\n",
    "initial guess ---> loop over states ---> update the values\n",
    "\n",
    "### 3.8. 实现 迭代侧率评估\n",
    "只要对于每个状态$ s\\in\\mathcal{S} ，v_\\pi(s)$是有限的，策略评估就保证会收敛于策略$ \\pi$  对应的状态值函数。\n",
    "\n",
    "对于有限的马尔可夫决策流程 (MDP)，只要满足以下条件之一，就保证会收敛：\n",
    "* $\\gamma < 1$ \n",
    "* 如果智能体以任何状态$ s\\in\\mathcal{S} $开始，并且遵守$ \\pi $，就保证会最终达到终止状态。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.10. 动作值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the state space and action space\n",
    "print(xx.observation_space)\n",
    "pirnt(xx.action_sapce)\n",
    "# print the total number of states and actions\n",
    "print(xx.nS)\n",
    "print(xx.nA)c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 迭代策略评估\n",
    "#### 动作值的估值\n",
    "#### 策略改进\n",
    "* an initial guess for the optimal pllicy\n",
    "* policy improvement\n",
    "    * convert the state value function to an action value function\n",
    "    * use the action-value function to obtain a policy that's better than the equal proable random policy\n",
    "\n",
    "\n",
    "#### 策略迭代\n",
    "\n",
    "\n",
    "#### 截断策略迭代\n",
    "the smaller of theta,the longer of time. how much longer depend on your MDP\n",
    "\n",
    "\n",
    "#### 值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"4\"></a>\n",
    "# [4.蒙特卡洛方法](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the agent is not given full knowledge of how the environmetn operates\n",
    "\n",
    "$S_0,A_0,R_1,S_1,A_1,R_2,....,S_T$\n",
    "\n",
    "Agent's Goal is to find  policy to maximize expected cumulative reward(最大化预期累计奖励）\n",
    "\n",
    "### 4.3. MC预测：状态值\n",
    "* On-Policy Method 异同策略方法\n",
    "\n",
    "**The Monte Carlo prediction algorithm** takes the average of these values and plugs it in as an estimate for the value of state X\n",
    "\n",
    "**visit**: every occurrence of a state in an episode  \n",
    "1.**first-visit MC method** : each episode only consider the **first visit** to the state and average those returns\n",
    "2.**every-visit MC method** :average the return following **all visits** to state in all episodes\n",
    "\n",
    "\n",
    "### 4.6. 预测：动作值\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 补充：\n",
    "\n",
    "### 与监督学习，非监督学习区别\n",
    "强化学习,主要基于环境environment和机器人agent之间的互动，环境没有给机器人提供什么标签数据之类的，而是给它一定的刺激reward，让它的行为发生一定的改变。我们稍微总结一下这三类学习的特点：\n",
    "\n",
    "* 监督学习：有标签的数据，通过学习出来一组好的参数来预测标签\n",
    "* 无监督学习：无标签数据，没有直接的反馈，更多的是挖掘数据中的隐含规律，与数据挖掘类似\n",
    "* 强化学习： \n",
    "    * 无特定的数据，只有奖励信号； \"\n",
    "    * 奖励信号不一定实时，大部分情况滞后； \n",
    "    * 研究的不是独立同分布的数据，更多的是时间序列的数据； \n",
    "    * 当前的行为影响后续的分布；\n",
    "    \n",
    "监督学习和无监督学习像是大学生考试前的死记硬背，硬是数据的分布和结构通过学习得到的参数背下来；而强化学习更多的像是人在做数学题时的逻辑推理，它当前的信号和刺激影响后来的信号和刺激。 \n",
    "\n",
    "强化学习的关键要素有：environment，reward，action 和 state。有了这些要素我们就能建立一个强化学习模型。强化学习解决的问题是，针对一个具体问题得到一个最优的policy，使得在该策略下获得的reward最大。所谓的policy其实就是一系列action。也就是sequential data。 \n",
    "\n",
    "### 根据环境分类\n",
    "* 不理解环境 （model-free RL）  \n",
    "1.机器人不懂得环境是什么样子，它会通过自己在环境中不断试错，以获得行为上的改变。   \n",
    "2.这类模型常用的有Q learning,Sarsa，Policy Gradients.\n",
    "\n",
    "* 理解环境的 （model-base RL）  \n",
    "1.机器人会通过先验的只是来先理解这个真实世界是怎么样子的，然后用一个模型来模拟现实世界的反馈，这样它就可以在它自己虚拟的世界中玩耍了。   \n",
    "2.与modle-free中的玩耍方式一样，但model-base有两个世界，不仅能在现实世界中玩耍，也能在自己虚拟的世界中玩耍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "n = int(input())\n",
    "i = 0\n",
    "a = 1\n",
    "while i <= n-1:\n",
    "    a = a*n\n",
    "    i += 1\n",
    "a = a-n+1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '5'></a>\n",
    "# [5.时间差分方法（Temporal Difference learning）](#0)\n",
    "> 学习如何应用时间差分方法（例如Sarsa、Q学习和预期）解决阶段性和连续性任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD预测：TD(0)\n",
    "- 修改更新动作值步骤\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TD预测：动作值\n",
    "### TD控制：Sarsa(0)\n",
    "### TD控制：Sarsamax\n",
    "### TD控制：预期Sarsa\n",
    "### 分析性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '6'></a>\n",
    "# [6 .连续空间中的强化学习](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL probblems are typically framed as Markov Decision Processers or MDPs: ($\\mathcal{S,A,P,R}, \\gamma)$\n",
    "<img src = \"./img/RL_Framework.png\" width =\"55%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

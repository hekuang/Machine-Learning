{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [课程1.聚类](#1)\n",
    "\n",
    "\n",
    "\n",
    "* [课程2： 层次聚类与密度聚类](#2)　　\n",
    "　\n",
    " \n",
    " \n",
    "* [课程 3：高斯混合模型与聚类验证](#3)　　\n",
    "\n",
    "\n",
    " \n",
    "* [课程 4：特征缩放](#4)\n",
    "\n",
    "\n",
    "* [课程 5： PCA（主成分分析)](#5)\n",
    "\n",
    "\n",
    "* [课程6 ：随机投影与ICA](#6)\n",
    "\n",
    "\n",
    "* [项目：创建客户细分](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# [1.聚类](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.\n",
    "> dataset without labels all the data points are of same class\n",
    "* clusters in the data 数据的聚类（簇）\n",
    "* dimensionality reducetion 降维\n",
    "\n",
    "### 1.4\n",
    "> 聚类算法如：K-Means   \n",
    "\n",
    "重复执行1.2\n",
    "1.assign 分配\n",
    "2.optimize 优化 降低距离中心总的二次距离b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# [2. 层次聚类与密度聚类](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.\n",
    "<img src = './img/K-means情况.png' width = '77%'>\n",
    "\n",
    "\n",
    "### 2.2.其他聚类方法\n",
    "* Hierarchical Clustering 层次聚类\n",
    "* Density_based Clustering 密度聚类 (well on the two-cresect dataset(双月牙形))\n",
    "   * 关注其中的DBSCAN(Density_based Spatial Clustering of Applications with Noise)\n",
    "<img src = \"./img/层次聚类and密度聚类.png\" width =\"88%\">\n",
    "\n",
    "### 2.3.层次聚类：单链接聚类法\n",
    " \n",
    "dendgrogram 系统树图\n",
    "<img src = \"./img/linkage_dendrogram.png\"    width=\"66%\">\n",
    "* single link clustering \n",
    "    * assume each point is a cluster\n",
    "    * calculate the distances and choose the smallest distance between two clusters. group those two points into a cluster\n",
    "**关注类的最短距离**    \n",
    "##### can give eithet one cluster or the number of our smaples     \n",
    "<img src = \"./img/single_link_clustering.png \" width = \"77%\">\n",
    "\n",
    "\n",
    "### 2.4. 检测单链接聚类\n",
    "\n",
    "\n",
    "<img src =\"./img/k_maeans_vs_single_linkA_clustering.png\"  width=\"77%\">\n",
    "\n",
    "### 2.5. 三种不同的层次聚类法\n",
    "> part of agglomerative clustering(凝聚聚类) component of Scikit-learn\n",
    "层次聚类包括凝聚聚类\n",
    "\n",
    "\n",
    "#### 凝聚聚类的三种方法\n",
    "> 没有单链接法\n",
    "\n",
    "*  **全连接法**   \n",
    " \n",
    "     **区别：关注类与类之间点的远距离中的最小值**\n",
    "\n",
    "* **组平均法（average link )**\n",
    "> 计算类之间点距离的平均值\n",
    "\n",
    "*  **Ward's metod(离差平方和法)**\n",
    "取类之间点的中间点，计算此点到所有点的距离平方和，然后减去类中的变量（类中取中点，计算与类中点的平方和）。\n",
    "最后将两类间最小的，聚类。\n",
    "<img src = \"./img/Ward's_method.png\" width='77%'>\n",
    "\n",
    "### 2.6. 层次聚类法的具体使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, cluster\n",
    "# Load dataset\n",
    "X = datasets.load_iris().data[:10]\n",
    "\n",
    "# linkage : {“ward”, “complete”, “average”}, optional, default: “ward”\n",
    "# n_clusters : int, default=2nThe number of clusters to find.\n",
    "clust = cluster.AgglomerativeClustering(n_clusters = 3, linkage = 'ward')\n",
    "\n",
    "labels = clust.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要画出层次树（系统树）\n",
    "hierarchical tree shapes called dendrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1715eab1668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, ward, single\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = datasets.load_iris().data[:10]\n",
    "# 用ward做聚类，传回关系矩阵\n",
    "linkage_matrix = ward(X)\n",
    "dendrogram(linkage_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化对聚类的影响\n",
    "如iris数据，第四列比其他小，方差影响对聚类的处理。由此进行标准化操作，使每个维度位于（0，1）  \n",
    "方法：每列减去最小值，初一范围，利用sklearn提供的preprocessing.normalize()工具\n",
    "` normalized_X = preprocessing.normalize(iris.data) `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.9 . HC 实例与应用\n",
    " **advantages**  \n",
    " * Resulting hierarchical represetation can be very informative\n",
    " * provides an additional ability to visualize\n",
    " * Especially potent when the dataset contains real hierarchical relationships(e.g. Evolutionary biology)  \n",
    " \n",
    "\n",
    " **disadvantages**  \n",
    " * Sensitive to noise and outliers(离散群）\n",
    " * Computationally inensive O(N^2)\n",
    " \n",
    " \n",
    "### 2.11. DBSCAN\n",
    "One of Density_based Clustering 密度聚类\n",
    "* Epsilon :Search distance around point\n",
    "* MinPts: Minimum number of points required to form a density cluster  \n",
    "==> Noise point(不符合的点)  \n",
    "==> Core point（符合的点）  \n",
    "==> Border poin（不符合的点，但是被某个core point包含进去的点）  \n",
    "\n",
    "\n",
    "<img src = \"./img/k_maeans_vs_DBSCAN.png\" width=\"77%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "    metric_params=None, min_samples=5, n_jobs=1, p=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, cluster\n",
    "X = datasets.load_iris().data\n",
    "\n",
    "db = cluster.DBSCAN(eps = 0.5, min_samples = 5)\n",
    "db.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.15 . DBSCAN 实例与应用\n",
    " **advantages**  \n",
    " * We don't need to specify the number of clusters\n",
    " * Flexibility in the shapes & sizes of clusters (能灵活的找到并分类各种形状和大小的类)\n",
    " * Able to deal with noise and ooutliers\n",
    "\n",
    " **disadvantages**  \n",
    " * Faces difficulty finding clusters of varying densities\n",
    " * Border points that are reachable from two clusers(随机访问, 所以能达到多个类的点,会被随机分配)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3'> </a>\n",
    "#  [3.高斯混合模型GMM与聚类验证](#0)　　\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Mixture Model Clustering  is a soft clustering algorithm\n",
    "> means every point and every sample in our dataset will belong to every cluster that we hava   \n",
    "> but would hava different levels of membership in each cluster\n",
    "<img src = \"./img/GaussianMixtureModelClustering.png\" width = \"66%\">\n",
    "\n",
    "### 3.3. 一维高斯分布\n",
    "> 高斯混合分布本身并不是高斯模型,可能是多个高斯模型的组合\n",
    "\n",
    "### 3.4. 二维高斯分布\n",
    "**Multivariate Gaussian distribution**\n",
    "more variables ,and each of them is a Gaussian distribution\n",
    "\n",
    "### 3.8 期望最大化算法概述\n",
    "1.Initialize K gaussian distributions\n",
    "2.soft-cluster data-\"expectation\" called the Expectation step or the E step\n",
    "3.re-estimate the Gaussians based on the soft clustering  called Maximization or M step\n",
    "4.evluate log-likelihood to check for convergence 评估对数似然来检查收敛\n",
    "> 不收敛，跳到第二部继续执行\n",
    "\n",
    "### 3.9-10. 期望最大化\n",
    "1.Initialize gaussian distributions like use k-means\n",
    "\n",
    "2.step2\n",
    "<img src = \"./img/gaussian_step2.png\" width = \"88%\">\n",
    "\n",
    "3. step3\n",
    "<img src = \"./img/gaussian_step3-1.png\" width = \"88%\">\n",
    "<img src = \"./img/gaussian_step3-2.png\" width = \"88%\">\n",
    "\n",
    "4.evaluate the log-likelihood\n",
    "\n",
    "$$ ln p (X\\mid\\mu,\\sigma^2) = \\sum_{i=1}^{N}ln(\\sum_{k=1}^{K}\\pi_kN(X_i\\mid\\mu_k,\\sigma{_k^2})) $$\n",
    "\n",
    "the **higher** this number is ,the more sure we are that the mixer model that we've generated(越能确定生成的混合模型可以负责创建数据)\n",
    "\n",
    "### 3.11. 期望最大化示例\n",
    "Initialization：Default(k-means)\n",
    "\n",
    "### 3.13 GMM 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn import datasets, mixture\n",
    "X = datasets.load_iris().data[:10]\n",
    "gmm = mixture.GaussianMixture(n_components = 3)\n",
    "gmm.fit(X)\n",
    "clustering = gmm.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.14. GMM 示例与应用\n",
    "\n",
    "**advantages**  \n",
    " * Soft-clustering (sample membership of multiple clusters)\n",
    " * Cluster shape flexibility\n",
    "   a cluster can contain another cluster inside of ti \n",
    " \n",
    "\n",
    "**disadvantages** \n",
    " * Sensitive to initialization values \n",
    " * Possible to converge to local optimum (局部最优)\n",
    " * lead convergence(收敛) slowly)  \n",
    " \n",
    " \n",
    " ### 3.15. 聚类分析过程\n",
    " DARA <<<--->>> feature selection/extraction(特征选择/特征提取)\n",
    "    <<<--->>> clustering algorithm selection&tuning:based on what you need to do and how your data looks \n",
    "    <<<--->>> clustering validation(聚类评价)：use indices(指数)\n",
    "    <<<--->>> results interpretation(聚类结果解释)\n",
    "    \n",
    "### 3.16. 聚类验证\n",
    "**indices**  \n",
    "* External indices :　use for data was originally labelled;score how a clustering method did\n",
    "* Internal indices :  not labelled specifically with unsupervised learning；　\n",
    "* Relative indices :　indicate which of two clustering structures is better in some sense\n",
    "> basically, all internal can server as relative indices  \n",
    "\n",
    "**defined by**\n",
    "* Compactness　：　how close of elements  in the same cluster \n",
    "* Separability　：how far between differernt clusters\n",
    "\n",
    "### 3.17 External indices 外部评价指标\n",
    "例如： Adjusted Rand Index\n",
    "[-1,1]   \n",
    "$$Rand \\; Index = \\frac{a+b} {2n}$$\n",
    "\n",
    "* a: number of pairs in the same cluster C and in the same cluster in K\n",
    "* b: number of pairs in a different cluster C and in a different cluster in K\n",
    "* n: number of samples/points\n",
    "\n",
    "### 3.19 Internal Indices\n",
    "例如：silhouette index (轮廓系数)\n",
    "[-1,1]  \n",
    "$$S_i = \\frac{b_i-a_i}{ max(a_i,b_i)}$$\n",
    "* a: average distance to other samples in the same cluster\n",
    "* b: average distance to samples in the **closet neighboring**  cluster\n",
    "\n",
    "should not use in cluster like  two rings datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4'> </a>\n",
    "# [4.特征缩放](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature scaling\n",
    "**避免一个特种覆盖另一种**\n",
    "### 4.7-8\n",
    "$0\\leq X^\\prime \\leq1$\n",
    "$$X^\\prime= \\frac{X-X_{min}}{X_{max} - X_{min}}$$\n",
    "advantage: can expect in the output (预估输出相对稳定)\n",
    "disadvantage: if have outlies in input（输入特征有异常数值，会很棘手）\n",
    "\n",
    "那种算法会受到特种缩放影响：(变量增大，值变大)\n",
    "* SVM : make one twice as big as the other,it counts for twice as much\n",
    "* K-means: aslo distance has exactly the same characterization\n",
    "* Decision Trees:no\n",
    "* Linear tegression: feature A donesn't effect feature B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.4166666666666667, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def featureScaling(arr):\n",
    "    a = max(arr)\n",
    "    b = min(arr)\n",
    "    list = []\n",
    "    for i in range(len(arr)):\n",
    "        x =((arr[i]-b)/(a-b))\n",
    "        list.append(x)\n",
    "    return list\n",
    "data = [115, 140, 175]\n",
    "print (featureScaling(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '5'></a>\n",
    "# [5 PCA（主成分分析)](#0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle COmponent Analysis\n",
    "\n",
    "\n",
    "\n",
    "### 5.1-5 数据维度\n",
    "\n",
    "### 5.6. 用于数据转换的PCA\n",
    "> PCA finds a new coordinate system that's obtained from the old one by translation and rotation only.（仅通过转化和轮换获取新坐标）\n",
    "* moves the x-axis into the priccipal axis of variation(most variation relative to all the data points(x轴变化大))\n",
    "* aligns the x axis with the principal axis of varitation*\n",
    "\n",
    "### 5.9\n",
    "\n",
    "when we write vectors in PCA,lowest output vectors is normalized to one \n",
    "\n",
    "<img src = \"./img/PCA_normalized.png\" width =\"88%\">\n",
    "\n",
    "### 5.10-11. find center and axis \n",
    "**find**  \n",
    "1.the center of the data   \n",
    "2.principle axis of variaton(主变量轴)  \n",
    "* PCA also returns an importance value,a spread value(散布值) of this axis.  \n",
    ">  spread value tends to be very large for the very  first axis fo variation and much smaller for the second axis fo variation if the spread is small.\n",
    "orthogonality vectors\n",
    "\n",
    "### 5.12 那些数据可以用于PCA ？？？？\n",
    "主轴占主导地位\n",
    "\n",
    "### 5.13.\n",
    "captures 捕获更多数据的，更占优势，轴方向数据更多\n",
    "\n",
    "### 5.14 . 可测量的特征与潜在的特征练习\n",
    "* decision tree classifier ：决策结构，每个决策都会导致一系列后果和附加决策\n",
    "* SVC ： 用于分类\n",
    "* linear regerssion : 用于预测数据\n",
    "\n",
    "### 5.14-15 压缩特征\n",
    "* SelectKBest(K为要保留的特征数量）\n",
    "* SelectPercentile \n",
    "\n",
    "### 5.17.复合特征 Composite Features\n",
    "> tend to turn a whole bunch of features into just a few.  \n",
    "\n",
    "1.principle component (lose a minimal amount of information)  \n",
    "2.Projection  \n",
    "<img src = \"./img/Projection.png\" width = \"33%\">\n",
    "\n",
    "### 5.18. 最大方差variance\n",
    "which direction is the data more spread out(分散)：longer line is going to be the direction of maximum variance\n",
    "\n",
    "### 5.19. 最大方差的优点-minimizing the information loss\n",
    "the Direction that has the **largest variance** in the data determines the priciple component of a dataset\n",
    "advantage： retains maximum amount of information in the  original data\n",
    "\n",
    "longer direction, more  information.lose\n",
    "\n",
    "### 5.23. 利用特征转换的PCA\n",
    "\n",
    "automatically combine them into new features and rank the relative powers fo those new features.\n",
    "\n",
    "Powerful unsupervised learning technique that you can use to fundamentally understand the latent features in you data.\n",
    "\n",
    "### 5.24.\n",
    "\n",
    "n_components == min(n_samples, n_features)\n",
    "\n",
    "### 5.25 PCA 回顾\n",
    "\n",
    "**transform input features into their principal components**\n",
    "\n",
    "**pricipal components are defined as the directions in the data that maximize the varicance**\n",
    "\n",
    "**pricipal componests are all perpendicular to each other in a sense (主成份相互垂直)，所以不相互影响**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-70957d904760>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-70957d904760>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    form sklearn.decomposition import PCA\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def doPCA():\n",
    "    form sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = 2)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pac = doPCA()\n",
    "# 方差比\n",
    "print(print(pca.explained_variance_ratio_) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.29. 何时使用PCA\n",
    "1. latent features might be showing up in the patterns in you data, figure out if there's a latent feature.  \n",
    "   > ex: measure who the big shots are at Enron(估量出Enron的大亨是谁 )\n",
    "  \n",
    "2. dimensionality reduction\n",
    "   * visualize high-dimensional data(project it down to the first two principal component,  plot that draw scatter point )\n",
    "   * reduce noise(throwing away the less important principle components)\n",
    "   * make other algorithms(regerssion, classification) work better because fewer inputs(like eigenfaces 特征脸）\n",
    "  \n",
    "  \n",
    "### 5.30 PCA for facial recognition\n",
    "\n",
    "* picture fo faces generally have high input dimensionality(many pixels)\n",
    "* gaces hava general patterns that could be cpatured in smaller number of dimensions(two eye on top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '6'></a>\n",
    "#  [6 .机投影与ICA](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 随机投影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '7'></a>\n",
    "# [项目：创建客户细分](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.数据探索\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

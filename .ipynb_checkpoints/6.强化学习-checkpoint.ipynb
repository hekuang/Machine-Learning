{
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [课程1.强化学习框架：问题](#1)\n",
    "\n",
    "\n",
    "\n",
    "* [课程2：强化学习框架：解决方案](#2)　　\n",
    "　\n",
    " \n",
    " \n",
    "* [课程 3：动态规划](#3)　　\n",
    "\n",
    "\n",
    " \n",
    "* [课程 4：特征缩放](#4)|\n",
    "\n",
    "\n",
    "* [课程 5： PCA（主成分分析)](#5)\n",
    "\n",
    "\n",
    "* [课程6 ：随机投影与ICA](#6)\n",
    "\n",
    "\n",
    "* [项目：创建客户细分](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '1'></a>\n",
    "# [1.强化学习框架：问题](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**动作action，状态state，奖励reward**\n",
    "* states : a context provided to the agent for choosing intelligent actions \n",
    "<img src = \"./img/强化学习中的智能体环境互动.png\" width = \"44%\">\n",
    "#### 设置，重新经历\n",
    "* 强化学习 (RL) 框架包含学习与其**环境**互动的**智能体**。\n",
    "* 在每个时间步，智能体都收到环境的**状态**（环境向智能体呈现一种情况），智能体必须选择相应的响应**动作**。一个时间步后，智能体获得一个**奖励**（环境表示智能体是否对该状态做出了正确的响应）和新的**状态**。\n",
    "* 所有智能体的目标都是**最大化预期累积奖励**，或在所有时间步获得的预期奖励之和。\n",
    " \n",
    "### 1.3阶段性任务和连续性任务\n",
    "* **任务**是一种强化学习问题\n",
    "* **episodic tasks** : a well-defined ending point( 起始点，结束点明确，每当智能体抵达**最终状态**,阶段性任务都会结束)\n",
    "* **continuing tasks** : go on forever,without end.\n",
    "\n",
    "\n",
    "### 1.6 奖励假设\n",
    "\n",
    "> 奖励假设：所有目标都可以构建为最大化（预期）累积奖励。\n",
    "\n",
    "Reward Hypothesis ： **maximization of expected cumulative reward (最大化期望累积奖励)**  rely on a prediction or an estimate\n",
    "\n",
    "\n",
    "### 1.7 目标与奖励\n",
    "\n",
    "<img  src =\"./img/目标与奖励.png\" width = \"33%\"  >\n",
    "<img src = \"./img/6.1.7.reward.png\" width = \"55%\">\n",
    "\n",
    "\n",
    "### 1.10.累积奖励\n",
    "return-G: the sum of rewards from the next time step  (后续时间步的奖励之和)\n",
    "* 在时间步t的回报是：$ G_t = R_{t+1}+ R_{t+2}+ R_{t+3}+....$\n",
    "### 1.11. 折扣汇报\n",
    "\n",
    "discounted return: particularly relevant to contimuing tasks(有连续性任务的关系最大)\n",
    "discount rata $\\gamma \\in [0,1]$ (set by youself to refine the goal)\n",
    "> 尤其，当我们指代“回报”时，并不一定就是\\gamma = 1γ=1，当我们指代“折扣回报”时，并不一定就是\\gamma < 1γ<1。 \n",
    "\n",
    "\n",
    "$$ G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+....=\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}$$\n",
    "\n",
    "### 1.12\n",
    "* 折扣率的选取带来更多奖励,\n",
    "* 折扣回报 $\\gamma$ 是你设置的值，以便进一步优化智能体的目标。\n",
    "* $它必须指定 0 \\leq \\gamma \\leq 1。$\n",
    "* $如果 \\gamma=0，智能体只关心最即时的奖励。$\n",
    "* $如果 \\gamma=1，回报没有折扣。$\n",
    "* $\\gamma 的值越大，智能体越关心遥远的未来。\\gamma 的值越小，折扣程度越大，在最极端的情况下，智能体只关心最即时的奖$\n",
    "\n",
    "\n",
    "### 1.13.MDP（Markov Decision Process）马尔科夫决策过程\n",
    "<img src = \"./img/MDP_definition.png\" width = \"66%\">\n",
    "* actions -- action space denote with a script A (手写体A)\n",
    "* states  -- state space  denote with a script S\n",
    "\n",
    "通常，状态空间$ \\mathcal S$ 是指所有非终止状态集合。  \n",
    "在连续性任务（例如在视频中介绍的回收任务）中，就相当于所有状态集合。  \n",
    "在阶段性任务中，我们使用 $\\mathcal{S}^+$ 表示所有状态（包括终止状态）集合  \n",
    "动作空间 $\\mathcal A$ 是指智能体可以采取的动作集合。  \n",
    "如果在某些状态下，只能采取部分动作，我们还可以使用$ \\mathcal{A}(s)$ 表示在状态 $s\\in\\mathcal{S}$下可以采取的动作集合  \n",
    "\n",
    "### 1.15 一步动态特性\n",
    "实际上，在任何状态$ S_{t}  和动作 A_{t} ，都可以使用该图判断智能体将如何确定下个状态 S_{t+1} 和奖励 R_{t+1}$\n",
    "\n",
    "在随机时间步t，智能体环境互动变成一系列的状态、动作和奖励。  \n",
    "当环境在时间步 t+1 对智能体做出响应时，它只考虑上一个时间步 $(S_t, A_t )$ 的状态和动作。  \n",
    "尤其是，它不关心再上一个时间步呈现给智能体的状态。（换句话说，环境不考虑任何 $ S_0, \\ldots, S_{t-1}$。）  \n",
    "并且，它不考虑智能体在上个时间步之前采取的动作。（换句话说，环境不考虑任何 $A_0, \\ldots, A_{t-1} $。）  \n",
    "此外，智能体的表现如何，或收集了多少奖励，对环境选择如何对智能体做出响应没有影响。（换句话说，环境不考虑任何 $ R_0, \\ldots, R_t $ 。)\n",
    "\n",
    "因此，我们可以通过指定以下设置完全定义环境如何决定状态和奖励\n",
    "$$p(s′,r∣s,a)≐P(S_{t+1}=s ′ ,R_{t+1}\t =r∣S_t\t =s,A_t\t =a)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2'></a>\n",
    "# [2. 强化学习框架：解决方案](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 策略policy $\\pi$\n",
    "**Definition**:    \n",
    "1, A deterministic policy is a mapping(映射) $\\pi : \\mathcal{S}---> \\mathcal{A}$\n",
    "* income  : state\n",
    "* outcome : action\n",
    "\n",
    "2, A stochastic policy(随机性策略) is a mapping:  $\\pi : \\mathcal{S} \\times \\mathcal{A} --> [0,1]$\n",
    "> allow the agent to choose actions randomly  \n",
    "\n",
    "$$ \\pi(a|s) = P (A_t = a| S_t = s)$$\n",
    "* income: staste-s,action-a\n",
    "* outcome: probability that the agent takes action a while in state s\n",
    "\n",
    "### 2.4. 网络世界实例\n",
    "<img src = \"./img/网络世界.png\" width = \"44%\">\n",
    "\n",
    "### 2.5. 状态值函数\n",
    "\n",
    "* **state-value function** :a function of the environment state \n",
    "> For each state, the **state-value function** yields the expected return ,if the agent started in that state,and then followed the policy for all times steps.(从当前状态开始，并遵循该策略可能会获得的回报)\n",
    "\n",
    "<img src=\"./img/state-value_function.png\" width=\"55%\">\n",
    "\n",
    "### 2.6.贝尔曼方程(1)\n",
    "\n",
    " \n",
    "#### 贝尔曼方程\n",
    "在这个网格世界示例中，一旦智能体选择一个动作，\n",
    "\n",
    "* 它始终沿着所选方向移动（而一般 MDP 则不同，智能体并非始终能够完全控制下个状态将是什么）\n",
    "* 可以确切地预测奖励（而一般 MDP 则不同，奖励是从概率分布中随机抽取的）。\n",
    " \n",
    "任何状态s的值可以计算为即时奖励r和下个状态（折扣）值s'的**和**。\n",
    "\n",
    " 一般 MDP，我们需要使用期望值，因为通常即时奖励和下个状态无法准确地预测。 奖励和下个状态是根据 MDP 的一步动态特性选择的。在这种情况下，奖励 r 和下个状态 s' 是从（条件性）概率分布 p(s',r|s,a) 中抽取的，贝尔曼预期方程（对于 $v_\\pi$）表示了任何状态 s 对于预期即时奖励和下个状态的预期值的值：\n",
    "$$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    "\n",
    "#### 计算预期值\n",
    "1.如果智能体的策略 $\\pi$ 是**确定性策略**，智能体在状态 s 选择动作 $\\pi(s)$，贝尔曼预期方程可以重写为两个变量 $(s'  和 r) $的和：\n",
    "$$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s'))$$\n",
    "\n",
    "在这种情况下，我们将奖励和下个状态的折扣值之和$ (r+\\gamma v_\\pi(s'))$ 与相应的概率$ p(s',r|s,\\pi(s)) $相乘，并将所有概率相加得出预期值。\n",
    "\n",
    "2.如果智能体的策略 $\\pi$  是**随机性策略**，智能体在状态 s 选择动作 a 的概率是$ \\pi(a|s$，贝尔曼预期方程可以重写为三个变量（s' 、r 和 a）的和：$$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))$$\n",
    "在这种情况下，我们将奖励和下个状态的折扣值之和$ (r+\\gamma v_\\pi(s'))$ 与相应的概率 $\\pi(a|s)p(s',r|s,a)$ 相乘，并将所有概率相加得出预期值。\n",
    "### 2.8. 最优性\n",
    "**Definition**: \n",
    "$\\pi\\geq\\pi^\\prime \\; only\\; if \\; v_{\\pi^\\prime}(s)\\geq v_{\\pi} \\; for\\;  all \\; \\mathcal{s}\\in \\mathcal{S}$\n",
    "\n",
    "An optimal policy(最优策略) $\\pi_*$ satisfies  $\\pi_* \\geq \\pi$  for all $\\pi$\n",
    "\n",
    "$\\pi$ is a optimal policy , mostly it's not the only optimal policy\n",
    "\n",
    "**The optimal state-value function :** all optimal policies have the **same value function** which we dnote by  $\\mathcal{V}_*$（最优状态值函数）\n",
    "\n",
    "### 2.9. 动作值函数\n",
    "* **action-value function** :a function of the environment state and the agent's action\n",
    "> For each state and action. the action-value function yields the expected return, if the agent starts in that state, **takes the action** ,and then follows the policy for all future time steps（做出预定动作后，从当前状态开始，并遵循该策略可能会获得的回报，reward包括第一步)\n",
    "\n",
    "**The optimal action-value function (最优动作值函数)**is denoted $\\mathcal{q}_*$\n",
    "\n",
    "<img src=\"./img/状态值函数vs动作值函数.png\" width = \"88%\">\n",
    "\n",
    "### 2.11.  最优策略\n",
    "\n",
    "<img src = \"./img/最优策略.png\" width=\"88%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13. 贝尔曼方程（2）\n",
    "\n",
    "#### 贝尔曼预期方程\n",
    "#### 贝尔曼最优性方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"3\"></a>\n",
    "# [3.动态规划](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 简介\n",
    "> 在动态规划设置中，智能体完全了解表示环境特性的马尔可夫决策流程 (MDP)。（这比强化学习设置简单多了，在强化学习设置中，智能体一开始不知道环境如何决定状态和奖励，必须完全通过互动学习如何选择动作。）\n",
    "\n",
    "### 3.2. 迷你项目\n",
    "`https://viewae1e8f24.udacity-student-workspaces.com/notebooks/Dynamic_Programming-zh.ipynb`\n",
    "\n",
    " ### 3.4-5 迭代方法\n",
    " \n",
    "directly solving the value of all  state  is more diffficult   \n",
    "so start off with a guess for the value of each state, and using an **iterative迭代 method** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. 迭代策略评估 iterative policy evaluation\n",
    "\n",
    "initial guess ---> loop over states ---> update the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
